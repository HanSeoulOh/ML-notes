# Reinforcement Learning Notes
###### Taken by: Han Du
###### Last Updated: April 2019
## Q-Learning Methods

## Policy Gradient Methods


### REINFORCE PG Algorithm
defined by:
\[
J(\theta) = \mathbb{E}_{\tau \sim p(\tau;\theta)}[r(\tau)]
\]

\[
\nabla_{\theta}J(\theta) = \int_{\tau}r(\tau)\nabla_{\theta}p(\tau;\theta)d\tau
\]

The above expression is intractable. **Why?**

We notice:

\[
\nabla_{\theta}p(\tau;\theta) = p(\tau;\theta)\frac{\nabla_{\theta}p(\tau;\theta)}{p(\tau;\theta)} = \nabla_{\theta}\log{p(\tau;\theta)}p(\tau;\theta)
\]

Thus:

\[
\nabla_{\theta}J(\theta) = \int_{\tau}r(\tau)\nabla_{\theta}\log{p(\tau;\theta)}p(\tau;\theta)d\tau
\]

\[
\nabla_{\theta}J(\theta) =
\mathbb{E}_{\tau \sim p(\tau;\theta)}[r(\tau)\nabla_{\theta}\log{p(\tau;\theta)}]
\]

We define our unbiased estimator:

\[
\nabla_{\theta}J(\theta) \approx \sum_{t \geq 0}r(\tau)\nabla_\theta\log\pi_\theta(a_t|s_t)
\]

Note: If $r(\tau)$ is high/low, probabilities of actions are pushed up/down

Enhance probabilities by future rewards:

\[
\nabla_{\theta}J(\theta) \approx \sum_{t \geq 0}{}(\sum_{t' \geq t }{r_{t'}})\nabla_\theta\log\pi_\theta(a_t|s_t)
\]

Use a discount factor to ignore delayed effects:


\[
\nabla_{\theta}J(\theta) \approx \sum_{t \geq 0}{}(\sum_{t' \geq t }{\gamma^{t'-t} r_{t'}})\nabla_\theta\log\pi_\theta(a_t|s_t)
\]

Observation: Raw values of trajectory isn't meaningful. If rewards are all positive, probabilities all continue to increase.

Can we penalize our trajectories based on whether or not the expectation was over or under?

Idea: Choose $b(s_t)$ := moving average of rewards from all trajectories


\[
\nabla_{\theta}J(\theta) \approx \sum_{t \geq 0}{}(\sum_{t' \geq t }{\gamma^{t'-t} r_{t'} - b(s_t)})\nabla_\theta\log\pi_\theta(a_t|s_t)
\]

Observation: $b(s_t)$ resembles the value function in Q-Learning
This is the derivation of AC methods!

We replace the reward of the state with the Q-function and replace the baseline reward with the V-function:


\[
\nabla_{\theta}J(\theta) \approx \sum_{t \geq 0}{}(\sum_{t' \geq t }{Q^{\pi\theta}(s_t, a_t) - V^{\pi\theta}(s_t)})\nabla_\theta\log\pi_\theta(a_t|s_t)
\]

### Actor-Critic methods
We can learn Q and V using Q-learning methods. Then we combine Policy Gradients and Q-Learning to train an **actor** (the policy) and a **critic** (the Q-function).

##### Observations:
- Actor is used as the decision maker
- Critic only needs to learn values of state-action pairs generated by the policy
- Can now introduce techniques from Q-learning
- Advantage function $A^{\pi}(s,a)$ := $Q^{\pi}(s,a)$ $-$ $V^{\pi}(s)$




### Terms Appendix

$\tau$ := trajectory i.e. a sequence of state-action transitions of arbitrary length

$\theta$ := weights of function approximator

### Proof Appendix


##### P1. State transition probability function is not needed for policy gradient methods

\[
p(\tau;\theta) = \prod_{t \geq 0}{p(s_{t+1}|s_{t}, a_{t})\pi_{\theta}(a_{t}|s_{t})}{}
\]

\[
\log{p(\tau;\theta)} = \sum_{t \geq 0} \log{p(s_{t+1}|s_{t},a_{t})} + \log{\pi_{\theta}(a_t|s_t)}
\]

\[
\nabla_\theta\log{p(\tau;\theta)} = \sum_{t \geq 0}\nabla_\theta\log\pi_\theta(a_t|s_t)
\]

##### P2. Introducing baseline does not create bias in the gradient estimator for PG Methods

https://arxiv.org/pdf/1506.02438.pdf
